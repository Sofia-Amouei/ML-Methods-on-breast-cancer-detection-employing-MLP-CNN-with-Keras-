{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing CNN on breast cancer detection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, BatchNormalization, Conv1D, MaxPool1D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers import LeakyReLU\n",
    "from sklearn import preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/breast-cancer-wisconsin-data/data.csv')\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "\tid\t    diagnosis\tradius_mean\t  texture_mean\t perimeter_mean\t  area_mean\t  smoothness_mean\tcompactness_mean\tconcavity_mean\t  concave_points_mean\t...\t  texture_worst\t   perimeter_worst\t  area_worst\tsmoothness_worst\tcompactness_worst\t concavity_worst\tconcave_points_worst\tsymmetry_worst\tfractal_dimension_worst\t  Unnamed: 32\n",
    "0\t842302\t        M\t       17.99\t     10.38\t         122.80\t        1001.0\t      0.11840\t        0.27760\t             0.3001\t            0.14710\t        ...\t      17.33\t            184.60\t        2019.0\t         0.1622\t              0.6656\t          0.7119\t           0.2654\t            0.4601\t            0.11890\t              NaN\n",
    "1\t842517\t        M\t       20.57\t     17.77\t         132.90\t        1326.0\t      0.08474\t        0.07864\t             0.0869\t            0.07017\t        ...\t      23.41\t            158.80\t        1956.0\t         0.1238\t              0.1866\t          0.2416\t           0.1860\t            0.2750\t            0.08902\t              NaN\n",
    "2\t84300903\tM\t       19.69\t     21.25\t         130.00\t        1203.0\t      0.10960\t        0.15990\t             0.1974\t            0.12790\t        ...\t      25.53\t            152.50\t        1709.0\t         0.1444\t              0.4245\t          0.4504\t           0.2430\t            0.3613\t            0.08758\t              NaN\n",
    "3\t84348301\tM\t       11.42\t     20.38\t         77.58\t        386.1\t      0.14250\t        0.28390\t             0.2414\t            0.10520\t        ...\t      26.50\t            98.87\t        567.7\t         0.2098\t              0.8663\t          0.6869\t           0.2575\t            0.6638\t            0.17300\t              NaN\n",
    "4\t84358402\tM\t       20.29\t     14.34\t         135.10\t        1297.0\t      0.10030\t        0.13280\t             0.1980\t            0.10430\t        ...\t      16.67\t            152.20\t        1575.0\t         0.1374\t              0.2050\t          0.4000\t           0.1625\t            0.2364\t            0.07678\t              NaN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 rows Ã— 33 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFINE THE DATASET AND THE ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "\n",
    "\n",
    "class dataset():\n",
    "    def __init__(self):\n",
    "        self.data = pd.read_csv(dataset_dir)\n",
    "        # self.data.drop(['id'],axis=1,inplace=True)\n",
    "\n",
    "    # define a function to do label encoding\n",
    "    def label_encoding(self):\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        self.data['diagnosis'] = le.fit_transform(self.data['diagnosis'])\n",
    "        return self.data\n",
    "    # Define a function to prepare X and Y out of the data\n",
    "\n",
    "    def prepare_data(self):\n",
    "        self.data = self.label_encoding()\n",
    "        self.X = self.data.drop(columns=['diagnosis', 'id', 'Unnamed: 32'], axis=1)\n",
    "        self.Y = self.data['diagnosis']\n",
    "        return self.X, self.Y\n",
    "    # Define a function to split the data into train and test, standard scalar them, and reshape them to fit the CNN\n",
    "\n",
    "    def data_preparation(self):\n",
    "        self.X, self.Y = self.prepare_data()\n",
    "        self.X_train, self.X_test, self.Y_train, self.Y_test = train_test_split(self.X, self.Y, test_size=0.2, random_state=42)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.X_train = self.scaler.fit_transform(self.X_train)\n",
    "        self.X_test = self.scaler.transform(self.X_test)\n",
    "        self.X_train = self.X_train.reshape(self.X_train.shape[0], self.X_train.shape[1], 1)\n",
    "        self.X_test = self.X_test.reshape(self.X_test.shape[0], self.X_test.shape[1], 1)\n",
    "        return self.X_train, self.X_test, self.Y_train, self.Y_test\n",
    "###############################\n",
    "# Define the CNN model: we are going to define a CNN model with the following architecture:\n",
    "# 1. Convolutional layer with 32 filters and a kernel size of 3\n",
    "# 2. Using Leaky Relu activation function is better than Relu on the output of the convolutional layer\n",
    "# 3. Using L1 regularization of factor 0.01 applied to the kernel matrix, since the input is features\n",
    "# and L1 regularization is a way of feature selection\n",
    "# 4. Batch Normalization\n",
    "# 5 . Dropout\n",
    "\n",
    "\n",
    "class CNN():\n",
    "    def __init__(self, X_train, X_test, Y_train, Y_test):\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.Y_train = Y_train\n",
    "        self.Y_test = Y_test\n",
    "        self.model = Sequential()\n",
    "\n",
    "    def model_architecture(self):\n",
    "        self.model.add(Conv1D(32, 2, input_shape=(30, 1), kernel_regularizer=keras.regularizers.l1(0.01), kernel_initializer='HeNormal'))\n",
    "        self.model.add(LeakyReLU(alpha=0.01))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dropout(0.2))\n",
    "        self.model.add(MaxPool1D(pool_size=2, padding='same'))\n",
    "        self.model.add(Conv1D(64, 2, kernel_regularizer=keras.regularizers.l1(0.01), kernel_initializer='HeNormal'))\n",
    "        self.model.add(LeakyReLU(alpha=0.01))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dropout(0.2))\n",
    "        self.model.add(MaxPool1D(pool_size=2, padding='same'))\n",
    "        self.model.add(Conv1D(128, 2, kernel_regularizer=keras.regularizers.l1(0.01), kernel_initializer='HeNormal'))\n",
    "        self.model.add(LeakyReLU(alpha=0.01))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dropout(0.2))\n",
    "        self.model.add(Conv1D(256, 2, kernel_regularizer=keras.regularizers.l1(0.01), kernel_initializer='HeNormal'))\n",
    "        self.model.add(LeakyReLU(alpha=0.01))\n",
    "        self.model.add(BatchNormalization())\n",
    "        self.model.add(Dropout(0.2))\n",
    "        self.model.add(GlobalAveragePooling1D())\n",
    "        self.model.add(Dense(128))\n",
    "        self.model.add(LeakyReLU(alpha=0.01))\n",
    "        self.model.add(Dropout(0.2))\n",
    "        self.model.add(Dense(32))\n",
    "        self.model.add(Dense(1, activation='sigmoid'))\n",
    "        return self.model\n",
    "\n",
    "\n",
    "    def plot_history_accuracy(self, history):\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        axs[0].plot(history.history['accuracy'], label='accuracy')\n",
    "        axs[0].plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "        axs[0].set_ylabel('Accuracy')\n",
    "        axs[0].set_xlabel('Epoch')\n",
    "        axs[0].set_title('Model Accuracy')\n",
    "        axs[0].legend(loc='lower right')\n",
    "        axs[1].plot(history.history['loss'], label='loss')\n",
    "        axs[1].plot(history.history['val_loss'], label='val_loss')\n",
    "        axs[1].set_ylabel('Loss')\n",
    "        axs[1].set_xlabel('Epoch')\n",
    "        axs[1].set_title('Model Loss')\n",
    "        axs[1].legend(loc='upper right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Define a function to train the model\n",
    "\n",
    "    def train_evaluation_model(self, BATCH_SIZE=32, EPOCHS=50):\n",
    "        self.model = self.model_architecture()\n",
    "        self.model.compile(optimizer='sgd', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        # Define a callback function to save the model for the best validation accuracy\n",
    "        callback = tf.keras.callbacks.ModelCheckpoint(filepath=SAVE_MODEL_DIR, monitor='val_accuracy', save_best_only=True, verbose=1)\n",
    "        # REDUCE LEARNING RATE ON PLATEU\n",
    "        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.8, patience=6, min_lr=0.00001, verbose=1)\n",
    "        # fit the model:\n",
    "        history = self.model.fit(self.X_train, self.Y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(self.X_test, self.Y_test), verbose=1, callbacks=[callback, reduce_lr])\n",
    "        # self.plot_history(history)\n",
    "        self.plot_history_accuracy(history)\n",
    "        predict_labels = (self.model.predict(self.X_test) > 0.5).astype(np.int32)\n",
    "        print(classification_report(self.Y_test, predict_labels))\n",
    "        ###\n",
    "        print(\"Accuracy on the test Step:\", accuracy_score(self.Y_test, predict_labels))\n",
    "        # show confusion matrix\n",
    "        conf_matrix = confusion_matrix(self.Y_test, predict_labels)\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cbar=False, xticklabels=['benign', 'malignant'], yticklabels=['benign', 'malignant'])\n",
    "\n",
    "    # Define a function to evaluate the model\n",
    "\n",
    "    # def evaluate_model(self):\n",
    "    #     self.model1 = self.model_architecture()\n",
    "    #     self.model1.load_weights(SAVE_MODEL_DIR)\n",
    "    #     predict_labels = (self.model1.predict(self.X_test) > 0.5).astype(np.int)\n",
    "    #     print(classification_report(self.Y_test, predict_labels))\n",
    "    #     ###\n",
    "    #     print(\"Accuracy on the test Step:\", accuracy_score(self.Y_test, predict_labels))\n",
    "    #     # show confusion matrix\n",
    "    #     conf_matrix = confusion_matrix(self.Y_test, predict_labels)\n",
    "    #     sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cbar=False, xticklabels=['benign', 'malignant'], yticklabels=['benign', 'malignant'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN VALIDATE AND TEST THE MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Initialize the dataset class\n",
    "    dataset_obj = dataset()\n",
    "    # Prepare the data\n",
    "    X, Y = dataset_obj.prepare_data()\n",
    "    print(X.shape, Y.shape)\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, Y_train, Y_test = dataset_obj.data_preparation()\n",
    "    # Initialize the CNN class\n",
    "    cnn_obj = CNN(X_train, X_test, Y_train, Y_test)\n",
    "    # Train the model\n",
    "    cnn_obj.train_evaluation_model(BATCH_SIZE=12, EPOCHS=150)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1/50\n",
    "15/15 [==============================] - 2s 29ms/step - loss: 1.3545 - accuracy: 0.4813 - val_loss: 0.6546 - val_accuracy: 0.5965\n",
    "Epoch 2/50\n",
    "15/15 [==============================] - 0s 10ms/step - loss: 0.9907 - accuracy: 0.6176 - val_loss: 0.6130 - val_accuracy: 0.8158\n",
    "Epoch 3/50\n",
    "15/15 [==============================] - 0s 10ms/step - loss: 0.7708 - accuracy: 0.6615 - val_loss: 0.5721 - val_accuracy: 0.9123\n",
    "Epoch 4/50\n",
    "15/15 [==============================] - 0s 10ms/step - loss: 0.5768 - accuracy: 0.7385 - val_loss: 0.5300 - val_accuracy: 0.9561\n",
    "Epoch 5/50\n",
    "15/15 [==============================] - 0s 10ms/step - loss: 0.4978 - accuracy: 0.7956 - val_loss: 0.4918 - val_accuracy: 0.9561\n",
    "Epoch 6/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.4815 - accuracy: 0.7802 - val_loss: 0.4530 - val_accuracy: 0.9386\n",
    "Epoch 7/50\n",
    "15/15 [==============================] - 0s 10ms/step - loss: 0.4173 - accuracy: 0.8264 - val_loss: 0.4160 - val_accuracy: 0.9474\n",
    "Epoch 8/50\n",
    "15/15 [==============================] - 0s 10ms/step - loss: 0.3477 - accuracy: 0.8571 - val_loss: 0.3808 - val_accuracy: 0.9474\n",
    "Epoch 9/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.3470 - accuracy: 0.8484 - val_loss: 0.3465 - val_accuracy: 0.9474\n",
    "Epoch 10/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.2912 - accuracy: 0.8813 - val_loss: 0.3143 - val_accuracy: 0.9474\n",
    "Epoch 11/50\n",
    "15/15 [==============================] - 0s 10ms/step - loss: 0.3115 - accuracy: 0.8879 - val_loss: 0.2860 - val_accuracy: 0.9561\n",
    "Epoch 12/50\n",
    "15/15 [==============================] - 0s 12ms/step - loss: 0.3096 - accuracy: 0.8747 - val_loss: 0.2611 - val_accuracy: 0.9474\n",
    "Epoch 13/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.2509 - accuracy: 0.8989 - val_loss: 0.2401 - val_accuracy: 0.9474\n",
    "Epoch 14/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.2715 - accuracy: 0.8967 - val_loss: 0.2207 - val_accuracy: 0.9474\n",
    "Epoch 15/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.2191 - accuracy: 0.9143 - val_loss: 0.2026 - val_accuracy: 0.9474\n",
    "Epoch 16/50\n",
    "15/15 [==============================] - 0s 8ms/step - loss: 0.2637 - accuracy: 0.9099 - val_loss: 0.1881 - val_accuracy: 0.9474\n",
    "Epoch 17/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.2640 - accuracy: 0.9121 - val_loss: 0.1753 - val_accuracy: 0.9474\n",
    "Epoch 18/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.2350 - accuracy: 0.8989 - val_loss: 0.1640 - val_accuracy: 0.9561\n",
    "Epoch 19/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.2204 - accuracy: 0.9231 - val_loss: 0.1535 - val_accuracy: 0.9561\n",
    "Epoch 20/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.1671 - accuracy: 0.9341 - val_loss: 0.1448 - val_accuracy: 0.9561\n",
    "Epoch 21/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.1699 - accuracy: 0.9319 - val_loss: 0.1374 - val_accuracy: 0.9561\n",
    "Epoch 22/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.2229 - accuracy: 0.9121 - val_loss: 0.1297 - val_accuracy: 0.9649\n",
    "Epoch 23/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.1935 - accuracy: 0.9121 - val_loss: 0.1228 - val_accuracy: 0.9649\n",
    "Epoch 24/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.1792 - accuracy: 0.9363 - val_loss: 0.1171 - val_accuracy: 0.9649\n",
    "Epoch 25/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.1770 - accuracy: 0.9429 - val_loss: 0.1096 - val_accuracy: 0.9649\n",
    "Epoch 26/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.1559 - accuracy: 0.9451 - val_loss: 0.1033 - val_accuracy: 0.9649\n",
    "Epoch 27/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.1734 - accuracy: 0.9253 - val_loss: 0.0981 - val_accuracy: 0.9649\n",
    "Epoch 28/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.1686 - accuracy: 0.9341 - val_loss: 0.0961 - val_accuracy: 0.9649\n",
    "Epoch 29/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.1596 - accuracy: 0.9473 - val_loss: 0.0928 - val_accuracy: 0.9649\n",
    "Epoch 30/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.1403 - accuracy: 0.9407 - val_loss: 0.0897 - val_accuracy: 0.9649\n",
    "Epoch 31/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.1138 - accuracy: 0.9516 - val_loss: 0.0876 - val_accuracy: 0.9649\n",
    "Epoch 32/50\n",
    "15/15 [==============================] - 0s 10ms/step - loss: 0.1602 - accuracy: 0.9385 - val_loss: 0.0859 - val_accuracy: 0.9649\n",
    "Epoch 33/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.1462 - accuracy: 0.9429 - val_loss: 0.0850 - val_accuracy: 0.9649\n",
    "Epoch 34/50\n",
    "15/15 [==============================] - 0s 10ms/step - loss: 0.1660 - accuracy: 0.9231 - val_loss: 0.0828 - val_accuracy: 0.9649\n",
    "Epoch 35/50\n",
    "15/15 [==============================] - 0s 10ms/step - loss: 0.1501 - accuracy: 0.9407 - val_loss: 0.0791 - val_accuracy: 0.9649\n",
    "Epoch 36/50\n",
    "15/15 [==============================] - 0s 10ms/step - loss: 0.1518 - accuracy: 0.9407 - val_loss: 0.0775 - val_accuracy: 0.9649\n",
    "Epoch 37/50\n",
    "15/15 [==============================] - 0s 10ms/step - loss: 0.1560 - accuracy: 0.9473 - val_loss: 0.0765 - val_accuracy: 0.9649\n",
    "Epoch 38/50\n",
    "15/15 [==============================] - 0s 10ms/step - loss: 0.1550 - accuracy: 0.9451 - val_loss: 0.0755 - val_accuracy: 0.9649\n",
    "Epoch 39/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.1330 - accuracy: 0.9626 - val_loss: 0.0751 - val_accuracy: 0.9649\n",
    "Epoch 40/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.1143 - accuracy: 0.9516 - val_loss: 0.0743 - val_accuracy: 0.9649\n",
    "Epoch 41/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.1402 - accuracy: 0.9473 - val_loss: 0.0735 - val_accuracy: 0.9561\n",
    "Epoch 42/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.1119 - accuracy: 0.9626 - val_loss: 0.0739 - val_accuracy: 0.9561\n",
    "Epoch 43/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.1351 - accuracy: 0.9560 - val_loss: 0.0728 - val_accuracy: 0.9561\n",
    "Epoch 44/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.1442 - accuracy: 0.9451 - val_loss: 0.0724 - val_accuracy: 0.9561\n",
    "Epoch 45/50\n",
    "15/15 [==============================] - 0s 10ms/step - loss: 0.1538 - accuracy: 0.9516 - val_loss: 0.0713 - val_accuracy: 0.9649\n",
    "Epoch 46/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.1512 - accuracy: 0.9473 - val_loss: 0.0720 - val_accuracy: 0.9649\n",
    "Epoch 47/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.1121 - accuracy: 0.9648 - val_loss: 0.0715 - val_accuracy: 0.9649\n",
    "Epoch 48/50\n",
    "15/15 [==============================] - 0s 11ms/step - loss: 0.1208 - accuracy: 0.9516 - val_loss: 0.0707 - val_accuracy: 0.9649\n",
    "Epoch 49/50\n",
    "15/15 [==============================] - 0s 10ms/step - loss: 0.1161 - accuracy: 0.9626 - val_loss: 0.0722 - val_accuracy: 0.9649\n",
    "Epoch 50/50\n",
    "15/15 [==============================] - 0s 9ms/step - loss: 0.1189 - accuracy: 0.9560 - val_loss: 0.0719 - val_accuracy: 0.9649"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning(history,epoch):\n",
    "    epoch_range = range(1,epoch+1)\n",
    "    plt.plot(epoch_range,history.history['accuracy'])\n",
    "    plt.plot(epoch_range,history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train','Val'],loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(epoch_range,history.history['loss'])\n",
    "    plt.plot(epoch_range,history.history['val_loss'])\n",
    "    plt.title('Model Loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train','Val'],loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*If validation accuracy is greater than Training accuracy it means the model isn't overfitting\n",
    "*unless and untill validation loss goes above the Training loss we can keep on Training our model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Model_Accuracy:"
    "![__results___19_0](https://user-images.githubusercontent.com/120085689/214452468-15a1da0b-bd2b-4c47-9ee2-fcf41082285d.png)\n",
    "\n",
    "Model_Loss:\n",
    "![__results___19_1](https://user-images.githubusercontent.com/120085689/214452710-c205ecfc-c590-455a-926f-e204a772532e.png)"
    "\n",
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.9649122807017544"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "cm=confusion_matrix(y_test,y_pred)\n",
    "sns.heatmap(cm, annot=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<AxesSubplot:>",
    "![__results___23_1](https://user-images.githubusercontent.com/120085689/214452994-8776d88c-5519-45f7-a407-52ed8590e896.png)"
    "\n",
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
